services:
  # LNPixels API
  api:
    build:
      context: ./lnpixels/api
    ports:
      - "127.0.0.1:3000:3000"
    env_file:
      - .env
    volumes:
      # Mount directory instead of individual files for SQLite WAL mode compatibility
      - ./data/lnpixels:/app/data
      - ./backups:/app/backups
    environment:
      - PORT=3000
      - NODE_ENV=production
      - POSTGRES_URL=postgresql://postgres:postgres@postgres:5432/pixel_agent
      - CORRELATOR_URL=http://narrative-correlator:3004
      - API_URL=http://api:3000/api
    networks:
      - pixel-net
    restart: unless-stopped
    healthcheck:
      test: [ "CMD", "wget", "--spider", "-q", "http://127.0.0.1:3000/api/stats" ]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 40s
    deploy:
      resources:
        limits:
          memory: 512M

  # Narrative Correlator Service
  narrative-correlator:
    build:
      context: ./services/narrative-correlator
      dockerfile: Dockerfile
    ports:
      - "127.0.0.1:3004:3004"
    env_file:
      - .env
    environment:
      - NODE_ENV=production
      - PORT=3004
    networks:
      - pixel-net
    volumes:
      - ./data:/data
    restart: unless-stopped
    healthcheck:
      test: [ "CMD", "wget", "--spider", "-q", "http://127.0.0.1:3004/correlations/health" ]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 20s
    deploy:
      resources:
        limits:
          memory: 512M

  # Temporal Precision Protocol Service
  temporal-precision:
    build:
      context: ./services/temporal-precision
      dockerfile: Dockerfile
    ports:
      - "127.0.0.1:3005:3005"
    env_file:
      - .env
    environment:
      - NODE_ENV=production
      - PORT=3005
      - DATA_PATH=/data
    networks:
      - pixel-net
    volumes:
      - ./data:/data
    restart: unless-stopped
    healthcheck:
      test: [ "CMD", "wget", "--spider", "-q", "http://127.0.0.1:3005/temporal/health" ]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 20s
    deploy:
      resources:
        limits:
          memory: 512M

  # Docu-Gardener Service - Automated Queue→Archive Sync
  docu-gardener:
    build:
      context: ./services/docu-gardener
      dockerfile: Dockerfile
    ports:
      - "3006:3006"
    env_file:
      - .env
    environment:
      - NODE_ENV=production
      - PORT=3006
      - POLL_INTERVAL_MS=300000
      - PIXEL_ROOT=/pixel
    networks:
      - pixel-net
    volumes:
      - .:/pixel
      - ./logs:/logs
    restart: unless-stopped
    healthcheck:
      test: [ "CMD", "wget", "--spider", "-q", "http://127.0.0.1:3006/health" ]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 20s
    deploy:
      resources:
        limits:
          memory: 256M

  # LNPixels Web App (Canvas)
  web:
    build:
      context: ./lnpixels/lnpixels-app
      args:
        - NEXT_PUBLIC_API_URL=${NEXT_PUBLIC_API_URL:-http://127.0.0.1:3000/api}
    ports:
      - "127.0.0.1:3002:3002"
    env_file:
      - .env
    environment:
      - NODE_ENV=production
      - HOSTNAME=0.0.0.0
    networks:
      - pixel-net
    depends_on:
      api:
        condition: service_healthy
      narrative-correlator:
        condition: service_started
    restart: unless-stopped
    healthcheck:
      test: [ "CMD", "wget", "--spider", "-q", "http://127.0.0.1:3002/" ]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 60s
    deploy:
      resources:
        limits:
          memory: 512M

  # Pixel Landing Page
  landing:
    build:
      context: ./pixel-landing
      args:
        - NEXT_PUBLIC_CANVAS_URL=${NEXT_PUBLIC_CANVAS_URL:-https://ln.pixel.xx.kg}
        - NEXT_PUBLIC_AGENT_HANDLE=${NEXT_PUBLIC_AGENT_HANDLE:-@PixelSurvivor}
        - NEXT_PUBLIC_LIGHTNING_ADDRESS=${NEXT_PUBLIC_LIGHTNING_ADDRESS:-sparepicolo55@walletofsatoshi.com}
        - NEXT_PUBLIC_BITCOIN_ADDRESS=${NEXT_PUBLIC_BITCOIN_ADDRESS:-bc1q7e33r989x03ynp6h4z04zygtslp5v8mcx535za}
    ports:
      - "127.0.0.1:3001:3001"
    env_file:
      - .env
    environment:
      - NODE_ENV=production
      - HOSTNAME=0.0.0.0
    networks:
      - pixel-net
    volumes:
      - ./pixel-landing/public/audit.json:/app/public/audit.json
      - ./pixel-landing/public/agent_memories.json:/app/public/agent_memories.json
      - ./pixel-landing/public/syntropy.json:/app/public/syntropy.json
      - ./syntropy-core/CONTINUITY.md:/app/public/CONTINUITY.md
    restart: unless-stopped
    healthcheck:
      test: [ "CMD", "wget", "--spider", "-q", "http://127.0.0.1:3001/" ]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 60s
    deploy:
      resources:
        limits:
          memory: 512M

  # Pixel Agent (Eliza)
  agent:
    build:
      context: ./pixel-agent
    ports:
      - "127.0.0.1:3003:3003"
    env_file:
      - .env
    volumes:
      - ./logs:/app/logs
      - ./data/eliza:/app/.eliza
    environment:
      - NODE_ENV=production
      - LNPIXELS_WS_URL=http://api:3000
      - PORT=3003
      - CI=true
      # Use external PostgreSQL instead of embedded PGLite to prevent corruption
      - POSTGRES_URL=postgresql://postgres:postgres@postgres:5432/pixel_agent
      - DATABASE_URL=postgresql://postgres:postgres@postgres:5432/pixel_agent
    networks:
      - pixel-net
    stop_grace_period: 30s
    depends_on:
      api:
        condition: service_healthy
      postgres:
        condition: service_healthy
    restart: unless-stopped
    healthcheck:
      test: [ "CMD", "wget", "--spider", "-q", "http://127.0.0.1:3003/health" ]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 120s
    deploy:
      resources:
        limits:
          memory: 2G

  # Syntropy Core (AI Orchestration)
  syntropy:
    build:
      context: ./syntropy-core
    # Run as host user to avoid root-owned files, but add docker group for socket access
    user: "1000:1000"
    group_add:
      - "${DOCKER_GID:-999}" # docker group GID
    env_file:
      - .env
    volumes:
      - .:/pixel
      - /var/run/docker.sock:/var/run/docker.sock
    environment:
      - NODE_ENV=production
      - PIXEL_ROOT=/pixel
      - PIXEL_AGENT_DIR=/pixel/pixel-agent
      - DB_PATH=/pixel/data/pixels.db
      - LOG_PATH=/pixel/logs/agent.log
      - AUDIT_LOG_PATH=/pixel/pixel-landing/public/audit.json
      - HOME=/tmp
      # HOST_PIXEL_ROOT is the absolute path on the host machine.
      # Syntropy passes this to docker compose when spawning workers.
      - HOST_PIXEL_ROOT=${HOST_PIXEL_ROOT:-${PWD}}
      # Git authentication via Personal Access Token
      - GH_TOKEN=${GH_TOKEN}
      # Security: Prevent worker spawn cascade (OOM protection)
      - MAX_CONCURRENT_WORKERS=${MAX_CONCURRENT_WORKERS:-2}
    networks:
      - pixel-net
    depends_on:
      api:
        condition: service_healthy
      agent:
        condition: service_started
    restart: unless-stopped
    healthcheck:
      test: [ "CMD", "curl", "-sf", "http://127.0.0.1:3000/health" ]
      interval: 60s
      timeout: 10s
      retries: 3
      start_period: 30s
    deploy:
      resources:
        limits:
          memory: 4G

  # Postgres Database - External DB for ElizaOS (prevents PGLite corruption)
  postgres:
    image: pgvector/pgvector:pg15
    ports:
      - "127.0.0.1:5432:5432"
    environment:
      - POSTGRES_DB=pixel_agent
      - POSTGRES_USER=postgres
      - POSTGRES_PASSWORD=postgres
    volumes:
      - ./data/postgres:/var/lib/postgresql/data
    networks:
      - pixel-net
    restart: unless-stopped
    healthcheck:
      test: [ "CMD-SHELL", "pg_isready -U postgres -d pixel_agent" ]
      interval: 10s
      timeout: 5s
      retries: 5
      start_period: 10s
    deploy:
      resources:
        limits:
          memory: 512M

  # pgAdmin - PostgreSQL Web GUI (not started by default)
  # Start manually: docker compose --profile tools up -d pgadmin
  # Access via SSH tunnel: ssh -L 5050:127.0.0.1:5050 user@vps
  pgadmin:
    profiles: [ "tools" ]
    image: dpage/pgadmin4:latest
    ports:
      - "127.0.0.1:5050:80"
    environment:
      - PGADMIN_DEFAULT_EMAIL=pixel@example.com
      - PGADMIN_DEFAULT_PASSWORD=pixel123
      - PGADMIN_CONFIG_SERVER_MODE=False
      - PGADMIN_CONFIG_MASTER_PASSWORD_REQUIRED=False
      - PGADMIN_SERVER_JSON_FILE=/pgadmin4/servers.json
    volumes:
      - ./data/pgadmin:/var/lib/pgadmin
      - ./data/pgadmin-servers.json:/pgadmin4/servers.json:ro
    networks:
      - pixel-net
    depends_on:
      postgres:
        condition: service_healthy
    restart: unless-stopped
    deploy:
      resources:
        limits:
          memory: 256M

  # Nginx Reverse Proxy
  nginx:
    image: nginx:alpine
    command: [ "nginx", "-g", "daemon off; error_log /dev/null emerg;" ]
    logging:
      driver: "none"
    ports:
      - "80:80"
      - "443:443"
    volumes:
      - ./nginx/nginx.conf:/etc/nginx/conf.d/default.conf:ro
      - ./certbot/conf:/etc/letsencrypt:ro
      - ./certbot/www:/var/www/certbot:ro
    networks:
      - pixel-net
    depends_on:
      - api
      - web
      - landing
    restart: unless-stopped
    healthcheck:
      test: [ "CMD", "wget", "--spider", "-q", "--no-check-certificate", "https://127.0.0.1/" ]
      interval: 30s
      timeout: 10s
      retries: 3

  # Certbot for SSL
  certbot:
    image: certbot/certbot
    volumes:
      - ./certbot/conf:/etc/letsencrypt
      - ./certbot/www:/var/www/certbot
    entrypoint: "/bin/sh -c 'trap exit TERM; while :; do certbot renew; sleep 12h & wait $${!}; done;'"
    restart: unless-stopped

  # Database Backup Service (runs daily at 3 AM)
  backup:
    image: alpine:latest
    volumes:
      - ./data:/data:ro
      - ./backups:/backups
    entrypoint: |
      /bin/sh -c '
      while true; do
        echo "[$(date)] Starting backup..."
        cp /data/pixels.db /backups/pixels_$(date +%Y%m%d_%H%M%S).db
        cp /data/db.sqlite /backups/agent_$(date +%Y%m%d_%H%M%S).db
        # Keep only last 7 days of backups
        find /backups -name "*.db" -mtime +7 -delete
        echo "[$(date)] Backup complete. Sleeping 24h..."
        sleep 86400
      done
      '
    restart: unless-stopped

  # Worker Container (spawned on-demand by Syntropy)
  # Not started by default - use docker compose run worker
  worker:
    build:
      context: ./syntropy-core
      dockerfile: Dockerfile.worker
    profiles:
      - worker # Not started by default
    user: "1000:1000"
    group_add:
      - "${DOCKER_GID:-999}"
    volumes:
      # IMPORTANT: Use HOST_PIXEL_ROOT to specify the absolute host path.
      # When Syntropy spawns workers via docker socket, relative paths (.) resolve
      # to the container's PWD, not the host's. This breaks volume mounts.
      - ${HOST_PIXEL_ROOT:-.}:/pixel
      # SECURITY NOTE: Docker socket grants host-level access. Workers need it
      # to run docker compose commands. Guardrails in worker-entrypoint.sh limit
      # what commands are permitted. Single-worker lock prevents spawn cascades.
      - /var/run/docker.sock:/var/run/docker.sock
    environment:
      - NODE_ENV=production
      - PIXEL_ROOT=/pixel
      - HOME=/tmp
      - CI=true
      - OPENCODE_TELEMETRY_DISABLED=true
      - OPENCODE_MODEL=${OPENCODE_MODEL:-opencode/glm-4.7-free}
      - WORKER_TIMEOUT_SECONDS=${WORKER_TIMEOUT_SECONDS:-2700}
      # TASK_ID is passed at runtime via -e TASK_ID=xxx
    env_file:
      - .env
    networks:
      - pixel-net
    restart: "no" # Ephemeral - no restart policy
    deploy:
      resources:
        limits:
          memory: 4G
          cpus: "2"

  # Opencode log tailer (makes worker output visible in `docker compose logs -f`)
  # This tails the shared log file that every worker appends to.
  # REASON FOR RENAME: Avoid name conflict with ephemeral workers (pixel-worker-*)
  opencode-logs:
    image: alpine:latest
    volumes:
      - .:/pixel
    working_dir: /pixel
    command: >-
      /bin/sh -c 'mkdir -p /pixel/logs && touch /pixel/logs/opencode_live.log && tail -n 0 -F /pixel/logs/opencode_live.log'
    restart: unless-stopped
    networks:
      - pixel-net

  # ============================================
  # VPS Metrics Collector (sidecar for Syntropy)
  # ============================================
  # Provides host-level resource visibility to the orchestrator.
  # Writes metrics every 60s to ./data/vps-metrics.json (atomic).
  #
  # Design: v2.0 - Incorporates Gemini review feedback
  # - Atomic writes via temp file + mv
  # - jq for safe JSON construction
  # - 10s timeout on docker stats
  # - Healthcheck on file age
  #
  vps-monitor:
    image: alpine:latest
    container_name: pixel-vps-monitor-1
    logging:
      driver: "none"
    volumes:
      # Read-only access to kernel interfaces
      - /proc:/host/proc:ro # CPU, memory, uptime
      - /:/host/root:ro # Disk usage via df
      # Docker socket for container stats
      - /var/run/docker.sock:/var/run/docker.sock:ro
      # Output directory (shared with syntropy)
      - ./data:/data
      # Mount the monitor script
      - ./scripts/vps-monitor.sh:/vps-monitor.sh:ro
    environment:
      - METRICS_INTERVAL=${VPS_METRICS_INTERVAL:-60}
      - METRICS_FILE=/data/vps-metrics.json
    entrypoint: /bin/sh
    command:
      - -c
      - |
        apk add --no-cache jq docker-cli >/dev/null 2>&1 || exit 1
        exec /bin/sh /vps-monitor.sh
    networks:
      - pixel-net
    restart: unless-stopped
    # Healthcheck: Verify file exists and is recent
    healthcheck:
      test: [ "CMD-SHELL", "[ -f /data/vps-metrics.json ] && [ $(($(date +%s) - $(stat -c %Y /data/vps-metrics.json))) -lt 180 ]" ]
      interval: 60s
      timeout: 10s
      retries: 3
      start_period: 60s
    deploy:
      resources:
        limits:
          memory: 128M
          cpus: "0.2"

  # Bitcoin Core (Testnet) - Backend for Lightning
  # OPTIMIZED: Reduced memory from 2G to 1.2G for Plan C scaling (2026-01-05)
  # HEALTH FIX: Updated health check to verify IBD completion (2026-01-05)
  # MEMORY OPTIMIZATION: Reduced memory footprint via T068 (2026-01-08)
  #   - dbcache: 400MB → 64MB
  #   - maxmempool: 100MB → 50MB
  #   - prune: 5000MB → 550MB (aggressive pruning)
  #   - Added par=1 for parallel transaction validation
  #   - mem_limit: 1200M → 900M
  #   - mem_reservation: 700M
  # Bitcoin Core (Testnet)
  # IMMUTABLE: DO NOT REDUCE PRUNE BELOW 5000 OR RAM BELOW 2G.
  # THIS IS REQUIRED FOR LIGHTNING STABILITY.
  bitcoin:
    image: ruimarinho/bitcoin-core:24
    container_name: pixel-bitcoin-1
    command: >
      bitcoind -testnet=1 -server=1 -rest=1 -rpcbind=0.0.0.0 -rpcallowip=0.0.0.0/0 -rpcuser=bitcoin -rpcpassword=password123 -zmqpubrawblock=tcp://0.0.0.0:28332 -zmqpubrawtx=tcp://0.0.0.0:28333 -prune=5000 -maxmempool=200 -dbcache=1000
    ports:
      - "18333:18333" # Testnet P2P
      - "127.0.0.1:18332:18332" # Testnet RPC (local only)
      - "28332:28332" # ZMQ raw blocks
      - "28333:28333" # ZMQ raw transactions
    volumes:
      - ./data/bitcoin:/home/bitcoin/.bitcoin # Bitcoin data directory
    networks:
      - pixel-net
    restart: unless-stopped
    healthcheck:
      test: [ "CMD-SHELL", "bitcoin-cli -testnet -rpcuser=bitcoin -rpcpassword=password123 getblockcount || exit 0" ]
      interval: 30s
      timeout: 10s
      retries: 10
      start_period: 7200s
    deploy:
      resources:
        limits:
          memory: 2G
          cpus: "1"
        reservations:
          memory: 700M

  # Core Lightning (CLN) - Testnet Deployment
  # Operational reserve: 25,000 sats from 79,014 sats treasury
  # Purpose: Lightning Network capabilities for economic sovereignty
  # BLOCKCHAIN STATE CONTINUITY (T083): Uses startup wrapper to validate Bitcoin block height
  lightning:
    image: elementsproject/lightningd:v24.11.2
    container_name: pixel-lightning-1
    depends_on:
      bitcoin:
        condition: service_healthy
    ports:
      - "9735:9735" # Lightning P2P port (testnet)
      - "127.0.0.1:9736:9736" # RPC port (local only for security)
    volumes:
      - ./data/lightning:/root/.lightning # Lightning data directory
      - ./data/bitcoin:/bitcoin-data:ro # Bitcoin data for bcli (read-only)
      - ./config/bitcoin/bitcoin.conf:/bitcoin.conf:ro # Bitcoin config for bcli
      - ./scripts/blockchain-state:/scripts/blockchain-state:ro # T083: State continuity wrapper
    environment:
      - LIGHTNINGD_NETWORK=testnet
      - LIGHTNINGD_RPC_PORT=9736
      - EXPOSE_TCP=true
      - LIGHTNINGD_DATA=/root/.lightning # T083: Explicit data directory
    entrypoint: ["/scripts/blockchain-state/lightning-startup-wrapper.sh"]
    command:
      - "--testnet"
      - "--bitcoin-rpcconnect=bitcoin"
      - "--bitcoin-rpcuser=bitcoin"
      - "--bitcoin-rpcpassword=password123"
      - "--bitcoin-rpcport=18332"
      - "--addr=0.0.0.0:9735"
    networks:
      - pixel-net
    restart: unless-stopped
    healthcheck:
      test: [ "CMD", "lightning-cli", "--testnet", "getinfo" ]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 60s
    deploy:
      resources:
        limits:
          memory: 1G
          cpus: "1"

networks:
  pixel-net:
    driver: bridge
