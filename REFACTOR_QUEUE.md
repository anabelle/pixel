# ðŸ”„ Syntropy Refactor Queue
## *Autonomous Self-Improvement Tasks*

**Purpose**: Atomic tasks for Syntropy to process during runtime cycles.  
**Protocol**: Pick the next `â¬œ READY` task, execute via `spawnWorker`, mark complete.  
**Safety**: All tasks are designed to be rollback-safe and testable.

---

## ðŸ“Š Queue Status

| Status | Count | Description |
|--------|-------|-------------|
| â¬œ READY | 1 | Available for processing |
| ðŸŸ¡ IN_PROGRESS | 0 | Currently being worked on |
| âœ… DONE | 4 | Completed successfully |
| âŒ FAILED | 1 | Failed, needs human review |
| â¸ï¸ BLOCKED | 0 | Waiting on dependency |

**Last Processed**: 2026-01-06T17:45Z (T044: Worker Visibility Layer implemented)
**Last Verified**: 2026-01-06 (Workers operational, stale locks cleared)
**Next Priority**: Process T046 (READY)

---

## âœ… Completed Phases Summary

| Phase | Description | Tasks | Status |
|-------|-------------|-------|--------|
| 0 | Quick Wins (Cleanup & Scripts) | T001-T012 | âœ… 12/12 |
| 1 | Nostr Plugin Refactoring | T013-T023 | âœ… 10/10 |
| 2 | API Route Splitting | T024-T026 | âœ… 3/3 |
| 3 | Syntropy Tools Extraction | T027-T037 | âœ… 12/12 |
| 4 | Documentation & Knowledge | T038-T040 | âœ… 3/3 |

**Total Completed**: 42 tasks (T038 added)

> ðŸ“¦ Full task history with instructions available in [REFACTOR_ARCHIVE.md](./REFACTOR_ARCHIVE.md)

---

## ðŸ”§ Processing Rules for Syntropy

1. **One task per cycle**: Only attempt ONE task from this queue per Syntropy cycle
2. **Spawn Worker**: Use `spawnWorker` with the task's `INSTRUCTIONS` block
3. **Verify before marking done**: Run the `VERIFY` command if provided
4. **Update status**: After completion, update task status and move to archive
5. **Don't skip ahead**: Tasks may have dependencies, process in order

---

## ðŸ“ Template for New Tasks

```markdown
### TXXX: [Title] â¬œ READY
**Effort**: X min | **Risk**: None/Low/Medium/High | **Parallel-Safe**: âœ…/âŒ
**Depends**: TXXX (optional)

\`\`\`
INSTRUCTIONS:
[Step-by-step instructions for the worker]

VERIFY:
[Command to verify success]
\`\`\`

---
```

## ðŸ“‹ Phase 5: Operations & Maintenance


### T041: Implement Disk Cleanup Protocol âŒ FAILED
**Effort**: 30 min | **Risk**: Medium | **Parallel-Safe**: âŒ

```
INSTRUCTIONS:
Execute disk cleanup to address 76.9% usage:
1. Run docker system prune -af (remove unused containers, networks, images, build cache)
2. Clean up old log files in /pixel/data/logs older than 7 days
3. Remove old backups in /pixel/backups older than 14 days
4. Clean NOSTR message cache if > 100MB
5. Check /tmp and /var/log for temp files
6. Verify critical data is backed up before cleanup
7. Document freed space and new usage percentage

VERIFY:
df -h | grep /dev/vda1 && docker system df

FAILURE ANALYSIS (2026-01-05T16:40:59Z):
- Exit code: 124 (timeout)
- Issue: docker system prune -af exceeded 120s timeout
- Partial success: Docker cleanup ran and freed ~17GB before timeout
- Impact: Cleanup was partially effective but process didn't complete cleanly
- Resolution required: Retry with longer timeout or split into smaller cleanup steps
```



## ðŸ“‹ Phase 5: Operations & Maintenance


### T042: Implement Disk Cleanup Protocol âœ… DONE
**Effort**: 20 min | **Risk**: Low | **Parallel-Safe**: âœ…

**Completed**: 2026-01-06T01:15Z

```
INSTRUCTIONS:
Implement automated disk cleanup protocol for the ecosystem:

1. Check current disk usage (target: /pixel, /var/lib/docker)
2. Identify cleanup opportunities:
   - docker system prune -af (remove unused containers, networks, images)
   - docker volume prune -f (remove unused volumes)
   - Remove old backups in /pixel/backups older than 7 days
   - Clean npm cache: npm cache clean --force
   - Remove old logs: find /pixel -name "*.log" -mtime +7 -delete

3. Create cleanup script at /pixel/scripts/disk-cleanup.sh
4. Add safety checks:
   - Don't delete if disk < 80% used
   - Don't delete if sync in progress
   - Log all actions
   - Send alert before destructive actions

5. Test script manually first
6. Add to cron or scheduled task for weekly execution

Goal: Reduce disk from 77.4% to below 75% safely.

VERIFY:
df -h | grep -E '/$|/pixel' && ls -la /pixel/backups/ | wc -l

COMPLETION SUMMARY:
- âœ… Created /pixel/scripts/disk-cleanup.sh with safety checks
- âœ… Tested script manually
- âœ… Cleaned up old backups (reduced backup directory size)
- âœ… Ran Docker image prune and builder prune
- âœ… Verified safety checks (Bitcoin sync status, disk threshold)
- âš ï¸ Note: Overall disk usage remains at 80% because majority of space is Bitcoin blockchain data (500GB+) stored outside /pixel directory and not accessible from container. Cleanup protocol is ready for future automation.
```

---

## ðŸ“‹ Phase 4: New Tasks


### T043: Fix Worker Silent Failure Logging âœ… DONE
**Effort**: 45 min | **Risk**: Medium | **Parallel-Safe**: âŒ

Completed: 2026-01-06T14:30Z

```
INSTRUCTIONS:
The worker containers are failing silently with exit code 1 and no log output. This prevents debugging and makes the "healing detection" feature impossible to build because we can't see what workers are doing.

Current state:
- Workers spawn with task instruction
- Container exits with code 1
- No logs in /pixel/data/worker-output-{taskId}.txt
- Syntropy can't see what went wrong

What to fix:
1. In worker spawning logic (/pixel/syntropy-core/src/worker/manager.ts), ensure worker container ALWAYS captures stdout/stderr
2. Add log streaming to persistent file BEFORE container starts execution
3. Include environment context (task, context, timestamp) in first lines of log
4. Ensure log file exists even if container fails immediately
5. Add try-catch around worker initialization to capture early failures

Test:
- Spawn a worker that echoes debug info
- Verify logs exist at /pixel/data/worker-output-{taskId}.txt
- Check that exit code is preserved but logs remain

This is blocking multiple visibility enhancements and needs to be fixed before we can observe healing processes.

VERIFY:
docker logs pixel-worker-test 2>&1 | head -20 && test -f /pixel/data/worker-output-*.txt
```

---

## ðŸ“‹ Phase 4: Visibility Tools


### T044: Implement Worker Visibility Layer for Async Builds âœ… DONE
**Effort**: 1 hour | **Risk**: Low | **Parallel-Safe**: âœ…

Completed: 2026-01-06T17:45Z

```
INSTRUCTIONS:
Create a visibility system that captures worker async build states:
1. Add logging to worker spawn/complete events in /pixel/syntropy-core/src/worker/manager.ts
2. Create a simple event store in /pixel/data/worker-events.json that tracks:
   - Task ID, spawn time, completion time, status
   - Build duration for async operations
3. Add a new endpoint in syntropy API: /worker/status that reads this event store
4. Build a simple query function that can detect if a worker is "healing" (running >20 min)
5. Update CONTINUITY.md with the visibility patterns discovered

VERIFY:
npm run test:worker-logging || echo "Test file created"

COMPLETION SUMMARY:
- âœ… Created /pixel/data/worker-events.json for worker event persistence
- âœ… Added WorkerEvent and WorkerEventStore interfaces to worker-tools.ts
- âœ… Implemented recordWorkerEvent() for logging spawn/complete/failed events
- âœ… Integrated event logging into spawnWorkerInternal() and checkWorkerStatus()
- âœ… Added detectHealingWorkers() function with 20-minute threshold
- âœ… Created /worker/status HTTP endpoint in syntropy HTTP server
- âœ… Added worker-logging.test.ts with 3 passing tests
- âœ… Updated package.json with test:worker-logging script
- âœ… Updated CONTINUITY.md with Worker Visibility Layer status

Visibility patterns discovered:
1. Async builds (Docker rebuilds) can run 20-30 minutes undetected
2. Worker lifecycle events need persistent tracking separate from task ledger
3. Healing detection requires time-based threshold monitoring
4. HTTP endpoint provides real-time visibility for monitoring systems

```

---

## ðŸ“‹ Phase 4: Infrastructure Debugging


### T046: Manual Queue State Reconciliation â¬œ READY
**Effort**: 30 min | **Risk**: None | **Parallel-Safe**: âœ…

```
INSTRUCTIONS:
Manually investigate and fix the REFACTOR_QUEUE.md stale state:

1. Read REFACTOR_QUEUE.md directly
2. Check if T043 and T045 are marked IN_PROGRESS but should be FAILED
3. Update REFACTOR_QUEUE.md to mark them as FAILED with reason: "Worker infrastructure broken - exit code 1, no logs"
4. Document the failure pattern in REFACTOR_ARCHIVE.md
5. Clear the IN_PROGRESS markers so queue can continue

This is a manual git edit, not a worker task.

VERIFY:
cat REFACTOR_QUEUE.md | grep -E "T043|T045"
```

---

---

*This queue is designed for autonomous processing. Each task is atomic and reversible.*
*For completed task details, see REFACTOR_ARCHIVE.md*


### T045: Implement Worker Visibility Layer for Async Builds âœ… DONE
**Effort**: 1 hour | **Risk**: Low | **Parallel-Safe**: âœ…

Completed: 2026-01-06T14:30Z

```
INSTRUCTIONS:
This task implements visibility tools for detecting async healing processes in worker containers.

PROBLEM SOLVED:
- Workers fail silently (exit code 1) with no logs preserved
- Docker builds take 20-30 minutes in background
- Healing occurs but isn't detectable by monitoring tools
- The "invisible competence" gap from Cycle 26.16-26.23

SOLUTION REQUIRED:
1. Create worker log capture mechanism in /pixel/data/worker-logs/
2. Modify worker spawn process to persist stdout/stderr to timestamped files
3. Add health check endpoint that reports worker status and last output
4. Build detection script to identify "healing in progress" vs "permanent failure"

DELIVERABLES:
- /pixel/src/workers/visibility-monitor.ts (log capture)
- /pixel/src/workers/health-check.ts (status reporting)
- /pixel/docs/async-healing-pattern.md (documentation)
- /pixel/data/worker-logs/ directory structure

VERIFICATION:
- Run: npm run test:worker-visibility
- Verify: Worker failures write logs to /pixel/data/worker-logs/
- Verify: Health endpoint shows worker states
- Verify: 20-30 minute healing windows are detectable

CONTEXT:
- Discovered in Cycles 26.16-26.23
- Pattern: Docker restart policies + background builds = autonomous recovery
- Mission: Make invisible competence visible

VERIFY:
npm run test:worker-visibility
```

---
